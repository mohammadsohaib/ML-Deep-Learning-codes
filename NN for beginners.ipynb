{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/data-clean/clean_train.csv')\ntest = pd.read_csv('../input/data-clean/clean_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tid=test['TransactionID']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing high correl features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = train.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.70)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"droplist=['id_23','id_27','id_33','id_30','id_34','id_16','id_31','id_28','id_29','id_15','id_35','id_36','id_37','id_38']\ntrain=train.drop(columns=droplist,axis=1)\ntrain=train.drop(columns=to_drop,axis=1)\ntest=test.drop(columns=to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop('Unnamed: 0',axis=1)\ntest=test.drop('Unnamed: 0',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list(train.select_dtypes(include=['float64']).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical = [col for col in list(train.select_dtypes(include=numerics)) if col in train.columns]\ncategorical = [col for col in list(train.select_dtypes(include='object')) if col in train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy = train\nnumerical.remove('isFraud')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_counts = {}\nfrom sklearn import preprocessing\nfor f in train.columns:\n    if  train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        category_counts[f] = len(list(lbl.classes_)) + 1\n#         test[f] = lbl.transform(list(test[f].values))  \ntrain= train.reset_index()\n# test = test.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in test.columns:\n    if  test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(test[f].values))\n        test[f] = lbl.transform(list(test[f].values))  \n#         test = test.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop('index',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_ = train[\"isFraud\"]\n# train = train.drop([\"isFraud\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nfor column in numerical:\n    scaler = StandardScaler()\n    if train[column].max() > 100 and train[column].min() >= 0:\n        train[column] = np.log1p(train[column])\n        test[column] = np.log1p(test[column])\n    scaler.fit(np.concatenate([train[column].values.reshape(-1,1), test[column].values.reshape(-1,1)]))\n    train[column] = scaler.transform(train[column].values.reshape(-1,1))\n    test[column] = scaler.transform(test[column].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_input_features(df):\n    X = {'numerical':np.array(df[numerical])}\n    for cat in categorical:\n        X[cat] = np.array(df[cat])\n    return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Model Implementation\n * Different combination of layers can be created to boost scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.optimizers import  Adam\nimport keras.backend as k\nfrom keras.optimizers import SGD\nimport graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model():\n    k.clear_session()\n\n    categorical_inputs = []\n    for cat in categorical:\n        categorical_inputs.append(Input(shape=[1], name=cat))\n\n    categorical_embeddings = []\n    for i, cat in enumerate(categorical):\n        categorical_embeddings.append(\n            Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \"_embed\")(categorical_inputs[i]))\n    \n    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n#     categorical_logits = Dropout(.5)(categorical_logits)\n\n    numerical_inputs = Input(shape=[train[numerical].shape[1]], name = 'numerical')\n    numerical_logits = Dropout(.1)(numerical_inputs)\n  \n    x = Concatenate()([\n        categorical_logits, \n        numerical_logits,\n    ])\n#     x = categorical_logits\n    x = BatchNormalization()(x)\n    x = Dense(128, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(128, activation = 'relu')(x)\n    x = Dropout(.4)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(128, activation = 'relu')(x)\n    x = Dropout(.4)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(128, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x=  BatchNormalization()(x)\n    x = Dense(128, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    out = Dense(1, activation = 'sigmoid')(x)\n    \n\n    model = Model(inputs=categorical_inputs + [numerical_inputs],outputs=out)\n    loss = \"binary_crossentropy\"\n    model.compile(optimizer=SGD(lr = 0.003), loss = loss)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val = train_test_split(train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = get_input_features(train)\nX_train = get_input_features(X_train)\nX_valid = get_input_features(X_val)\nX_test = get_input_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n\ny_train, y_valid = train_test_split(train_copy[target], test_size=0.2, random_state=42)\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=10, patience=1, mode='auto', verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = make_model()\nbest_score = 0\npatience = 0\nfor i in range(100):\n    if patience < 4:\n        hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 512, epochs = 1, verbose = 1)\n        valid_preds = model.predict(X_valid, batch_size = 512, verbose = True)\n        score = roc_auc_score(y_valid, valid_preds)\n        print(score)\n        if score > best_score:\n            model.save_weights(\"model.h5\")\n            best_score = score\n            patience = 0\n        else:\n            patience += 1\n            pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import LearningRateScheduler\n# Creating the model\nmodel = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(100, \n                activation='relu',  \n                input_dim=111,\n                kernel_initializer='uniform'))\n#The argument being passed to each Dense layer (18) is the number of hidden units of the layer. \n# A hidden unit is a dimension in the representation space of the layer.\n\n#Stacks of Dense layers with relu activations can solve a wide range of problems\n#(including sentiment classification), and you’ll likely use them frequently.\n\n# Adding an Dropout layer to previne from overfitting\nmodel.add(Dropout(0.50))\nmodel.add(BatchNormalization())\n#adding second hidden layer \nmodel.add(Dense(128,\n                kernel_initializer='uniform',\n                activation='relu'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.50))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128,\n                kernel_initializer='uniform',\n                activation='relu'))\n\n# model.add(Dense(256,\n#                 kernel_initializer='uniform',\n#                 activation='relu'))\n# model.add(BatchNormalization())\nmodel.add(Dropout(0.50))\nmodel.add(BatchNormalization())\n# model.add(Dense(128,\n#                 kernel_initializer='uniform',\n#                 activation='relu'))\n# model.add(BatchNormalization())\n# Adding another Dropout layer\n# model.add(Dropout(0.50))\n\n# adding the output layer that is binary [0,1]\nmodel.add(Dense(1,\n                kernel_initializer='uniform',\n                activation='sigmoid'))\n#With such a scalar sigmoid output on a binary classification problem, the loss\n#function you should use is binary_crossentropy\nannealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n#Visualizing the model\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating an Stochastic Gradient Descent\nsgd = SGD(lr = 0.02, momentum = 0.9)\n\n# Compiling our model\nmodel.compile(optimizer = 'Adam',\n                   loss = 'binary_crossentropy', \n                   metrics = ['accuracy'])\n#optimizers list\n#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# Fitting the ANN to the Training set\n# model.fit(X_train, y_train, \n#                batch_size = 128, \n#                epochs = 30, verbose=2)\nmodel.fit(X_train, y_train, nb_epoch=15, batch_size=64, validation_split=0.2, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = np.hstack(y_pred)\nprint(len(z))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['TransactionID'] = tid\nsub['isFraud'] = z","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below implementation is from [Mobius](http://https://www.kaggle.com/arashnic)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers.normalization import BatchNormalization\n# from scipy import optimize\n\n# model = Sequential()\n\n# model.add(Dense(100,input_dim=141,kernel_initializer='uniform',\n#                 activation='relu'))\n\n# model.add(Dropout(0.40))\n\n# model.add(BatchNormalization())\n\n# # model.add(Activation('relu'))\n\n# model.add(Dense(100),activation='relu')\n\n# model.add(Dropout(0.40))\n\n# model.add(BatchNormalization())\n\n# # model.add(Activation('relu'))\n\n# model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n\n# model.compile(optimizer=Adam(lr=0.02), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n# annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import roc_auc_score\n# from keras.callbacks import Callback\n\n# class printAUC(Callback()):\n    \n#     def __init__(self, X_train, y_train):\n\n#         super(printAUC, self).__init__()\n\n#         self.bestAUC = 0\n\n#         self.X_train = X_train[0]\n\n#         self.y_train = y_train[0]\n\n\n#     def on_epoch_end(self, epoch, logs={}):\n\n#         pred = self.model.predict(np.array(self.X_train))\n\n#         auc = roc_auc_score(self.y_train, pred)\n\n#         print(\"Train AUC: \" + str(auc))\n\n#         pred = self.model.predict(self.validation_data[0])\n\n#         auc = roc_auc_score(self.validation_data[1], pred)\n\n#         print (\"Validation AUC: \" + str(auc))\n#         if (self.bestAUC < auc) : \n\n#             self.bestAUC = auc\n\n#             self.model.save(\"bestNet.h5\", overwrite=True)\n\n#         return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit(X_train,y_train, batchsize=32, epochs = 30, callbacks=[annealer, printAUC(X_train, y_train)], validationdata = (X_val,Y_val), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = model.evaluate(X_train, y_train, batch_size=30)\n# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[['TransactionID','isFraud']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Fit the model\n# history = model.fit(X_train, y_train, validation_split=0.20, \n#                     epochs=18, batch_size=10, verbose=0)\n\n# # list all data in history\n# print(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # summarizing historical accuracy\n# plt.plot(history.history['acc'])\n# plt.plot(history.history['val_acc'])\n# plt.title('Model Accuracy')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # summarize history for loss\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some great kernels to learn from-"},{"metadata":{},"cell_type":"markdown","source":"* https://www.kaggle.com/mirichoi0218/ann-making-model-for-binary-classification\n* https://www.kaggle.com/parthsuresh/binary-classifier-using-keras-97-98-accuracy\n* https://www.kaggle.com/kabure/titanic-eda-keras-nn-pipelines\n* https://www.kaggle.com/karthik7395/binary-classification-using-neural-networks/data\n* https://www.kaggle.com/harnalashok/deep-learning-for-binary-classification\n* https://www.kaggle.com/deepthiappam/keras-binary-classification-neural-networks\n* https://www.kaggle.com/c/avito-demand-prediction/discussion/59917\n* http://blog.kaggle.com/2018/01/18/an-intuitive-introduction-to-generative-adversarial-networks/\n* https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d\n* http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}